---
layout: post
title: Spark基础知识整理
---
## SparkSQL vs Hive on Spark

基本一件东西。SparkSQL仅仅使用Hive metastore提高速度。使用Spark的SQL处理器。Hive on Spark使用Spark执行引擎，自己的SQL处理器。

Spark aimed for fast realtime interactive analytics. need large amount of memory. unified stack for streaming, interactive and predictive analysis. RDD distributed table.

- driver program (spark context)
- worker node

spark context to connect to the cluster
spark session a unified entrypoint (spark context, streaming, sql)
spark applications are seperated.
deploy mode client: driver outside of the cluster, cluster mode: driver inside of the cluster
task->stage->job
stage consists of a collection of tasks running same code on different set of data. a stage is without shuffle.

- no execution occurs until an action
- rdd recomputed with each action use persist to retain in memeory

DataFrame is RDD in tabular format, with query optimization.

flatmap applies to every element that returns multiple element into a new rdd.

persist DataFrame by temporary view or shared table.

inferred schema performance overhead

PySpark is built on top of Spark's Java API. Data is processed in Python and cached /shuffled in the JVM
