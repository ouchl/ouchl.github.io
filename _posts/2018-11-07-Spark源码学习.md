---
layout: post
title: Spark源码学习
---

#### 准备环境
load-spark-env.sh调用load-spark.sh，通过set -a将load-spark里所有的变量export出来。
command -v java确定Java是否安装。
if [ "$num_jars" -eq "0" -a -z "$SPARK_ASSEMBLY_JAR" ]; 判断jar包数量为零并且不存在目录。
while IFS= read -d '' -r ARG; do 
IFS：internal field seperator 使用NULL character分隔。

#### 部署
 master负责协调，worker负责处理task。spark应用程序启动driver，driver和master通信。worker有一个worker进程负责管理本机的任务，每一个execute对应一个execute runner。executor执行具体任务。
- master进程管理整个集群
- worker进程管理本地的任务执行器
- driver进程运行app主函数并创建sparkcontext
- 任务执行器持有任务进程池
- 每个执行器只处理一个app


#### job逻辑执行图
定义了job的数据依赖，即job有哪些RDD以及RDD之间的依赖关系。本质是计算链。依赖问题包括
- RDD之间的依赖关系
- RDD有多少个分区
- 分区之间的依赖关系
分区一般由用户指定，不指定的话根据转换确定。全部依赖是真依赖，一个分区依赖另一个分区的全部数据。部分依赖是一个分区依赖另一个分区的部分数据。

#### job物理执行图
一个app包含多个job，每个job包含多个stage，每个stage包含多个task。

 #### RDD依赖
 完全依赖 NarrowDependency 部分依赖ShuffleDependency shuffle的作用是做聚集操作 重新partition
 cogroup == full outer join 
 join == inner join
 partitioner不同可能导致shuffle
 coalesce减少partition数量 shuffle = true等价于repartition，通过增加随机key保证每个partition的大小相等。

 #### map和mapPartitions的区别
 map对每一条记录进行操作参数是元素，mapPartitions将迭代器送入自定义函数，函数决定处理方式，参数是迭代器。
