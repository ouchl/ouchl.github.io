---
layout: post
title: Spark源码学习
---

#### 准备环境
load-spark-env.sh调用load-spark.sh，通过set -a将load-spark里所有的变量export出来。
command -v java确定Java是否安装。
if [ "$num_jars" -eq "0" -a -z "$SPARK_ASSEMBLY_JAR" ]; 判断jar包数量为零并且不存在目录。
while IFS= read -d '' -r ARG; do 
IFS：internal field seperator 使用NULL character分隔。

#### 部署
 master负责协调，worker负责处理task。spark应用程序启动driver，driver和master通信。worker有一个worker进程负责管理本机的任务，每一个execute对应一个execute runner。executor执行具体任务。

 #### RDD依赖
 完全依赖 NarrowDependency 部分依赖ShuffleDependency shuffle的作用是做聚集操作 重新partition
 cogroup == full outer join 
 join == inner join
 partitioner不同可能导致shuffle
 coalesce减少partition数量 shuffle = true等价于repartition，通过增加随机key保证每个partition的大小相等。

 #### map和mapPartitions的区别
 map对每一条记录进行操作参数是元素，mapPartitions将迭代器送入自定义函数，函数决定处理方式，参数是迭代器。
